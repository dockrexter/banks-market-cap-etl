# Largest Banks ETL Pipeline

This project implements a complete ETL (Extract–Transform–Load) pipeline that:

- Scrapes a Wikipedia page containing a table of the **world's largest banks by market capitalization**.
- Transforms the data, including currency conversion using an exchange-rate CSV file.
- Loads the cleaned data into both a CSV file and a SQLite database, and runs example SQL queries.

## Features

- **Extraction**: Reads the “List of largest banks” table from an archived Wikipedia URL using pandas HTML parsing.
- **Transformation**:
  - Renames columns (e.g., `Bank name` → `Name`, `Market cap (US$ billion)` → `MC_USD_Billion`).
  - Joins with an external `exchange_rate.csv` file to compute additional columns such as `MC_EUR_Billion` and `MC_GBP_Billion`.
  - Cleans string values (e.g., newline characters) and converts them to numeric types.
- **Loading**:
  - Saves the transformed data to `Largest_banks_data.csv`.
  - Loads the data into a SQLite database (`Banks.db`) as a table (`Largest_banks`).
  - Runs example analytical SQL queries.

## Project Structure

```text
.
├── banks_project.py         # Main ETL script (extract, transform, load, queries)
├── exchange_rate.csv        # Currency exchange rates used for transformation
├── Largest_banks_data.csv   # Output CSV generated by the pipeline
├── Banks.db                 # SQLite database created by the pipeline
├── code_log.txt             # Log file tracking ETL progress
├── README.md                # Project documentation
└── LICENSE                  # Project license (MIT recommended)
